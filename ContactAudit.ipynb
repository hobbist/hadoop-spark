{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[active: string, last_modified_date: timestamp, policy_id: string, reason: string]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime,timedelta\n",
    "from pyspark.sql import Row\n",
    "dt=datetime.now()\n",
    "rdd=sc.parallelize([Row(last_modified_date=dt,policy_id='CR_1',active='N',reason='removed'),\n",
    "                    Row(last_modified_date=dt+timedelta(minutes=5),policy_id='CR_2',active='Y',reason='applied'),\n",
    "                    Row(last_modified_date=dt+timedelta(minutes=6),policy_id='CR_1',active='Y',reason='applied'),\n",
    "                    Row(last_modified_date=dt+timedelta(minutes=6),policy_id='CR_1',active='N',reason='removed'),\n",
    "                    Row(last_modified_date=dt+timedelta(minutes=6),policy_id='CR_1',active='N',reason='removed')])\n",
    "df=rdd.toDF()\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+---------+------+-------+------------+\n",
      "| added|removed|  last_modified_date|policy_id|active| reason| currentEnts|\n",
      "+------+-------+--------------------+---------+------+-------+------------+\n",
      "|    []| [CR_1]|2018-07-08 22:39:...|     CR_1|     N|removed|          []|\n",
      "|[CR_2]|     []|2018-07-08 22:44:...|     CR_2|     Y|applied|      [CR_2]|\n",
      "|[CR_1]|     []|2018-07-08 22:45:...|     CR_1|     Y|applied|[CR_2, CR_1]|\n",
      "|    []| [CR_1]|2018-07-08 22:45:...|     CR_1|     N|removed|      [CR_1]|\n",
      "|    []| [CR_1]|2018-07-08 22:45:...|     CR_1|     N|removed|          []|\n",
      "+------+-------+--------------------+---------+------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag ,when,array,lit,udf\n",
    "from pyspark.sql import Column\n",
    "from pyspark.sql.types import ArrayType,StringType,NullType\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def addValue(arr,elem):\n",
    "    if elem not in arr:\n",
    "        arr.append(elem)\n",
    "    return arr\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def mergeArrays(arr1,arr2):\n",
    "    returnVal=[]\n",
    "    if arr1 != None:\n",
    "        returnVal.extend(arr1)\n",
    "    if arr2 != None:\n",
    "        returnVal.extend(arr2)\n",
    "    return returnVal\n",
    "\n",
    "##sqlContext.udf.register(f=addValue,name='addValue',returnType=ArrayType(StringType()))\n",
    "\n",
    "windowSpec=Window.partitionBy().orderBy('last_modified_date')\n",
    "df=df.withColumn('added',array()).withColumn('removed',array())\n",
    "df=df.select(when(df.active=='Y',addValue(df.added,df.policy_id)).otherwise(df.added).alias('added'),\n",
    "             when(df.active=='N',addValue(df.removed,df.policy_id)).otherwise(df.removed).alias('removed'),\n",
    "             df.last_modified_date,df.policy_id,df.active,df.reason)\n",
    "#df.schema\n",
    "df=df.withColumn('currentEnts',mergeArrays(lag('added').over(windowSpec),df.added))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = spark.createDataFrame([(\"Alive\", 4)], [\"Name\", \"Number\"])\\n\\n\\ndef example(n):\\n    return Row(\\'Out1\\', \\'Out2\\')(n + 2, n - 2)\\n\\n\\nschema = StructType([\\n    StructField(\"Out1\", IntegerType(), False),\\n    StructField(\"Out2\", IntegerType(), False)])\\n\\nexample_udf = f.UserDefinedFunction(example, schema)\\n\\nnewDF = df.withColumn(\"Output\", example_udf(df[\"Number\"]))\\nnewDF = newDF.select(\"Name\", \"Number\", \"Output.*\")\\n\\nnewDF.show(truncate=False)\\n\\n'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df = spark.createDataFrame([(\"Alive\", 4)], [\"Name\", \"Number\"])\n",
    "\n",
    "\n",
    "def example(n):\n",
    "    return Row('Out1', 'Out2')(n + 2, n - 2)\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Out1\", IntegerType(), False),\n",
    "    StructField(\"Out2\", IntegerType(), False)])\n",
    "\n",
    "example_udf = f.UserDefinedFunction(example, schema)\n",
    "\n",
    "newDF = df.withColumn(\"Output\", example_udf(df[\"Number\"]))\n",
    "newDF = newDF.select(\"Name\", \"Number\", \"Output.*\")\n",
    "\n",
    "newDF.show(truncate=False)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
