{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name| id|\n",
      "+----+---+\n",
      "|   1|  3|\n",
      "|   2|  4|\n",
      "|   3|  5|\n",
      "|   4|  6|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##WithColumn example\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "rdd=sc.parallelize([1,2,3,4])\n",
    "rdd.collect()\n",
    "rowRdd=rdd.map(lambda x: Row(name=x))\n",
    "schema = StructType([StructField(\"name\", IntegerType(), False)])\n",
    "df=sqlContext.createDataFrame(data=rowRdd,schema=schema)\n",
    "df2=df.select('name')\n",
    "df3=df.withColumn('id',df.name+2)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|avg(value1)|\n",
      "+---+-----------+\n",
      "|  b|        2.0|\n",
      "|  a|        2.0|\n",
      "+---+-----------+\n",
      "\n",
      "+-----------+---+\n",
      "|avg(value1)| id|\n",
      "+-----------+---+\n",
      "|        2.0|  b|\n",
      "|        2.0|  a|\n",
      "+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datafrm=sc.parallelize([Row(id='a',value1=1,value2=2),Row(id='b',value1=2,value2=3),Row(id='a',value1=3,value2=4)]).toDF()\n",
    "computedOne=datafrm.withColumn('newCol',datafrm.value1*datafrm.value2)\n",
    "averagedDataFrame=datafrm.groupBy('id').avg('value1')\n",
    "averagedDataFrame.show()\n",
    "sqlContext.registerDataFrameAsTable(df=datafrm,tableName='values')\n",
    "avgDF=sqlContext.sql('select avg(value1),id from values group by id')\n",
    "avgDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-------------------+\n",
      "|                dat|name|           prevDate|\n",
      "+-------------------+----+-------------------+\n",
      "|2018-04-20 00:00:00|   a|2018-04-19 00:00:00|\n",
      "|2018-04-20 00:00:00|   b|2018-04-19 00:00:00|\n",
      "|2018-04-20 00:00:00|   a|2018-04-19 00:00:00|\n",
      "+-------------------+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Calculating previous Date in dataFrame\n",
    "from datetime import datetime,timedelta\n",
    "from pyspark.sql.functions import when,date_add,col\n",
    "dateDF=sc.parallelize([Row(name='a',dat=datetime(2018,4,20)),\n",
    "                       Row(name='b',dat=datetime(2018,4,20)),\n",
    "                       Row(name='a',dat=datetime(2018,4,20))]).toDF()\n",
    "addedColumn=dateDF.withColumn('prevDate',when(dateDF.dat.isNotNull(),date_add(col('dat'),-1)).otherwise(dateDF['dat']))\n",
    "addedColumn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n",
      "+-------+\n",
      "|avg(id)|\n",
      "+-------+\n",
      "|    7.5|\n",
      "+-------+\n",
      "\n",
      "+---+---+\n",
      "| id| id|\n",
      "+---+---+\n",
      "|  1|  6|\n",
      "|  1|  7|\n",
      "|  1|  8|\n",
      "|  1|  9|\n",
      "|  2|  6|\n",
      "|  2|  7|\n",
      "|  2|  8|\n",
      "|  2|  9|\n",
      "|  3|  6|\n",
      "|  3|  7|\n",
      "|  3|  8|\n",
      "|  3|  9|\n",
      "|  4|  6|\n",
      "|  4|  7|\n",
      "|  4|  8|\n",
      "|  4|  9|\n",
      "|  5|  6|\n",
      "|  5|  7|\n",
      "|  5|  8|\n",
      "|  5|  9|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+------------------+------------------+\n",
      "|summary|                id|                id|\n",
      "+-------+------------------+------------------+\n",
      "|  count|                36|                36|\n",
      "|   mean|               5.0|               7.5|\n",
      "| stddev|2.6186146828319083|1.1338934190276817|\n",
      "|    min|                 1|                 6|\n",
      "|    max|                 9|                 9|\n",
      "+-------+------------------+------------------+\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "++\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "##range to generate df\n",
    "rangeDF=spark.range(1,10,1)\n",
    "rangeDF.show()\n",
    "##data frame select and where function\n",
    "filteredDF=rangeDF.select('id').where(rangeDF.id>5)\n",
    "filteredDF.show()\n",
    "#agg function\n",
    "maxId=filteredDF.agg(F.avg(filteredDF.id))\n",
    "maxId.show()\n",
    "##approxquantile\n",
    "#quartDF=rangeDF.approxQuantile('id',[0,1],0.5)\n",
    "#quartDF.show()\n",
    "rangeDF.columns\n",
    "\n",
    "##cross join\n",
    "crossDF=rangeDF.crossJoin(filteredDF)\n",
    "crossDF.show()\n",
    "crossDF.describe().show()\n",
    "crossDF.drop('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+----+\n",
      "|EmpName|EmpNo|Salary|rank|\n",
      "+-------+-----+------+----+\n",
      "|    Som| E125|  4000|   1|\n",
      "|    Som| E125|  6000|   2|\n",
      "|    Tom| E123|  2000|   1|\n",
      "|    Tom| E123|  2000|   1|\n",
      "|    Tom| E123|  8000|   2|\n",
      "|    Rom| E124|  3000|   1|\n",
      "|    Rom| E124|  7000|   2|\n",
      "|    Pom| E126|  5000|   1|\n",
      "+-------+-----+------+----+\n",
      "\n",
      "+-----+-------+------+-------+-----+------+\n",
      "|EmpNo|EmpName|Salary|EmpRank|EmpNo|Salary|\n",
      "+-----+-------+------+-------+-----+------+\n",
      "| E123|    Tom|  2000|      1| E123|  2000|\n",
      "| E124|    Rom|  3000|      1| E124|  3000|\n",
      "| E125|    Som|  4000|      1| E125|  4000|\n",
      "| E126|    Pom|  5000|      1| E126|  5000|\n",
      "| E125|    Som|  6000|      2| E125|  6000|\n",
      "| E124|    Rom|  7000|      2| E124|  7000|\n",
      "| E123|    Tom|  8000|      2| E123|  8000|\n",
      "| E123|    Tom|  2000|      1| E123|  2000|\n",
      "+-----+-------+------+-------+-----+------+\n",
      "\n",
      "+-----+-------+------+----+\n",
      "|EmpNo|EmpName|Salary|rank|\n",
      "+-----+-------+------+----+\n",
      "| E126|    Pom|  5000|   1|\n",
      "| E125|    Som|  4000|   1|\n",
      "| E125|    Som|  6000|   2|\n",
      "| E124|    Rom|  3000|   1|\n",
      "| E124|    Rom|  7000|   2|\n",
      "| E123|    Tom|  2000|   1|\n",
      "| E123|    Tom|  2000|   1|\n",
      "| E123|    Tom|  8000|   2|\n",
      "+-----+-------+------+----+\n",
      "\n",
      "+-----+-------+------+----+\n",
      "|EmpNo|EmpName|Salary|rank|\n",
      "+-----+-------+------+----+\n",
      "| E126|    Pom|  5000|   1|\n",
      "| E125|    Som|  4000|   1|\n",
      "| E125|    Som|  6000|   2|\n",
      "| E124|    Rom|  3000|   1|\n",
      "| E124|    Rom|  7000|   2|\n",
      "| E123|    Tom|  2000|   1|\n",
      "| E123|    Tom|  2000|   1|\n",
      "| E123|    Tom|  8000|   2|\n",
      "+-----+-------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def rowKeyFunction(x):\n",
    "    return x.Salary\n",
    "\n",
    "def ranking(x):\n",
    "    y=[]\n",
    "    for index,element in enumerate(x):\n",
    "        dictEle=element.asDict()\n",
    "        if(index==0):\n",
    "            dictEle['rank']=1\n",
    "        elif element.Salary==x[index-1].Salary:\n",
    "            dictEle['rank']=y[index-1]['rank']\n",
    "        else:\n",
    "            dictEle['rank']=y[index-1]['rank']+1\n",
    "        y.append(Row(**dictEle))\n",
    "    return y\n",
    "        \n",
    "empDf=sqlContext.read.csv(path='/home/kapil/software-apps/input-files/employee-input.csv',header=True,inferSchema=True)\n",
    "empDf.registerTempTable('employee')\n",
    "##rdd approach\n",
    "empRdd=empDf.rdd;\n",
    "windowPartitonRdd=empRdd.groupBy(lambda x:x['EmpNo'])\n",
    "sortedRdd=windowPartitonRdd.mapValues(lambda y:sorted(y,key=rowKeyFunction))\n",
    "rankedRdd=sortedRdd.mapValues(ranking)\n",
    "rowRdd=rankedRdd.values().flatMap(lambda x:x)\n",
    "createdDf=rowRdd.toDF()\n",
    "createdDf.show()\n",
    "\n",
    "\n",
    "##by usig window function\n",
    "windowSpec=Window.partitionBy(empDf['EmpNo']).orderBy(empDf['Salary'])\n",
    "##with joining two dataframes\n",
    "windowedDF=empDf.distinct().select(func.rank().over(windowSpec).alias('EmpRank'),empDf.EmpNo,empDf.Salary)\n",
    "joinedDF=empDf.join(other=windowedDF,on=([windowedDF.EmpNo==empDf.EmpNo,windowedDF.Salary==empDf.Salary]),how='inner')\n",
    "joinedDF.show()\n",
    "\n",
    "\n",
    "\n",
    "## using with column function and dense rank\n",
    "withColumnDf=empDf.withColumn(col=func.dense_rank().over(windowSpec),colName='rank')\n",
    "withColumnDf.show()\n",
    "\n",
    "\n",
    "##using sql\n",
    "sqlDf=sqlContext.sql('select EmpNo,EmpName,Salary, dense_rank() OVER (PARTITION BY EmpNo ORDER BY Salary ASC) as rank from employee')\n",
    "sqlDf.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
