{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name| id|\n",
      "+----+---+\n",
      "|   1|  3|\n",
      "|   2|  4|\n",
      "|   3|  5|\n",
      "|   4|  6|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##WithColumn example\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "rdd=sc.parallelize([1,2,3,4])\n",
    "rdd.collect()\n",
    "rowRdd=rdd.map(lambda x: Row(name=x))\n",
    "schema = StructType([StructField(\"name\", IntegerType(), False)])\n",
    "df=sqlContext.createDataFrame(data=rowRdd,schema=schema)\n",
    "df2=df.select('name')\n",
    "df3=df.withColumn('id',df.name+2)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|avg(value1)|\n",
      "+---+-----------+\n",
      "|  b|        2.0|\n",
      "|  a|        2.0|\n",
      "+---+-----------+\n",
      "\n",
      "+-----------+---+\n",
      "|avg(value1)| id|\n",
      "+-----------+---+\n",
      "|        2.0|  b|\n",
      "|        2.0|  a|\n",
      "+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datafrm=sc.parallelize([Row(id='a',value1=1,value2=2),Row(id='b',value1=2,value2=3),Row(id='a',value1=3,value2=4)]).toDF()\n",
    "computedOne=datafrm.withColumn('newCol',datafrm.value1*datafrm.value2)\n",
    "averagedDataFrame=datafrm.groupBy('id').avg('value1')\n",
    "averagedDataFrame.show()\n",
    "sqlContext.registerDataFrameAsTable(df=datafrm,tableName='values')\n",
    "avgDF=sqlContext.sql('select avg(value1),id from values group by id')\n",
    "avgDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-------------------+\n",
      "|                dat|name|           prevDate|\n",
      "+-------------------+----+-------------------+\n",
      "|2018-04-20 00:00:00|   a|2018-04-19 00:00:00|\n",
      "|2018-04-20 00:00:00|   b|2018-04-19 00:00:00|\n",
      "|2018-04-20 00:00:00|   a|2018-04-19 00:00:00|\n",
      "+-------------------+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime,timedelta\n",
    "from pyspark.sql.functions import when,date_add,col\n",
    "dateDF=sc.parallelize([Row(name='a',dat=datetime(2018,4,20)),\n",
    "                       Row(name='b',dat=datetime(2018,4,20)),\n",
    "                       Row(name='a',dat=datetime(2018,4,20))]).toDF()\n",
    "addedColumn=dateDF.withColumn('prevDate',when(dateDF.dat.isNotNull(),date_add(col('dat'),-1)).otherwise(dateDF['dat']))\n",
    "addedColumn.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
