{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name| id|\n",
      "+----+---+\n",
      "|   1|  3|\n",
      "|   2|  4|\n",
      "|   3|  5|\n",
      "|   4|  6|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##WithColumn example\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "rdd=sc.parallelize([1,2,3,4])\n",
    "rdd.collect()\n",
    "rowRdd=rdd.map(lambda x: Row(name=x))\n",
    "schema = StructType([StructField(\"name\", IntegerType(), False)])\n",
    "df=sqlContext.createDataFrame(data=rowRdd,schema=schema)\n",
    "df2=df.select('name')\n",
    "df3=df.withColumn('id',df.name+2)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|avg(value1)|\n",
      "+---+-----------+\n",
      "|  b|        2.0|\n",
      "|  a|        2.0|\n",
      "+---+-----------+\n",
      "\n",
      "+-----------+---+\n",
      "|avg(value1)| id|\n",
      "+-----------+---+\n",
      "|        2.0|  b|\n",
      "|        2.0|  a|\n",
      "+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datafrm=sc.parallelize([Row(id='a',value1=1,value2=2),Row(id='b',value1=2,value2=3),Row(id='a',value1=3,value2=4)]).toDF()\n",
    "computedOne=datafrm.withColumn('newCol',datafrm.value1*datafrm.value2)\n",
    "averagedDataFrame=datafrm.groupBy('id').avg('value1')\n",
    "averagedDataFrame.show()\n",
    "sqlContext.registerDataFrameAsTable(df=datafrm,tableName='values')\n",
    "avgDF=sqlContext.sql('select avg(value1),id from values group by id')\n",
    "avgDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-------------------+\n",
      "|                dat|name|           prevDate|\n",
      "+-------------------+----+-------------------+\n",
      "|2018-04-20 00:00:00|   a|2018-04-19 00:00:00|\n",
      "|2018-04-20 00:00:00|   b|2018-04-19 00:00:00|\n",
      "|2018-04-20 00:00:00|   a|2018-04-19 00:00:00|\n",
      "+-------------------+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Calculating previous Date in dataFrame\n",
    "from datetime import datetime,timedelta\n",
    "from pyspark.sql.functions import when,date_add,col\n",
    "dateDF=sc.parallelize([Row(name='a',dat=datetime(2018,4,20)),\n",
    "                       Row(name='b',dat=datetime(2018,4,20)),\n",
    "                       Row(name='a',dat=datetime(2018,4,20))]).toDF()\n",
    "addedColumn=dateDF.withColumn('prevDate',when(dateDF.dat.isNotNull(),date_add(col('dat'),-1)).otherwise(dateDF['dat']))\n",
    "addedColumn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n",
      "+-------+\n",
      "|avg(id)|\n",
      "+-------+\n",
      "|    7.5|\n",
      "+-------+\n",
      "\n",
      "+---+---+\n",
      "| id| id|\n",
      "+---+---+\n",
      "|  1|  6|\n",
      "|  1|  7|\n",
      "|  1|  8|\n",
      "|  1|  9|\n",
      "|  2|  6|\n",
      "|  2|  7|\n",
      "|  2|  8|\n",
      "|  2|  9|\n",
      "|  3|  6|\n",
      "|  3|  7|\n",
      "|  3|  8|\n",
      "|  3|  9|\n",
      "|  4|  6|\n",
      "|  4|  7|\n",
      "|  4|  8|\n",
      "|  4|  9|\n",
      "|  5|  6|\n",
      "|  5|  7|\n",
      "|  5|  8|\n",
      "|  5|  9|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+------------------+------------------+\n",
      "|summary|                id|                id|\n",
      "+-------+------------------+------------------+\n",
      "|  count|                36|                36|\n",
      "|   mean|               5.0|               7.5|\n",
      "| stddev|2.6186146828319083|1.1338934190276817|\n",
      "|    min|                 1|                 6|\n",
      "|    max|                 9|                 9|\n",
      "+-------+------------------+------------------+\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "++\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "##range to generate df\n",
    "rangeDF=spark.range(1,10,1)\n",
    "rangeDF.show()\n",
    "##data frame select and where function\n",
    "filteredDF=rangeDF.select('id').where(rangeDF.id>5)\n",
    "filteredDF.show()\n",
    "#agg function\n",
    "maxId=filteredDF.agg(F.avg(filteredDF.id))\n",
    "maxId.show()\n",
    "##approxquantile\n",
    "#quartDF=rangeDF.approxQuantile('id',[0,1],0.5)\n",
    "#quartDF.show()\n",
    "rangeDF.columns\n",
    "\n",
    "##cross join\n",
    "crossDF=rangeDF.crossJoin(filteredDF)\n",
    "crossDF.show()\n",
    "crossDF.describe().show()\n",
    "crossDF.drop('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+----+\n",
      "|EmpName|EmpNo|Salary|rank|\n",
      "+-------+-----+------+----+\n",
      "|    Som| E125|  4000|   1|\n",
      "|    Som| E125|  6000|   2|\n",
      "|    Tom| E123|  2000|   1|\n",
      "|    Tom| E123|  2000|   1|\n",
      "|    Tom| E123|  8000|   2|\n",
      "|    Rom| E124|  3000|   1|\n",
      "|    Rom| E124|  7000|   2|\n",
      "|    Pom| E126|  5000|   1|\n",
      "+-------+-----+------+----+\n",
      "\n",
      "+-----+-------+------+-------+-----+------+\n",
      "|EmpNo|EmpName|Salary|EmpRank|EmpNo|Salary|\n",
      "+-----+-------+------+-------+-----+------+\n",
      "| E123|    Tom|  2000|      1| E123|  2000|\n",
      "| E124|    Rom|  3000|      1| E124|  3000|\n",
      "| E125|    Som|  4000|      1| E125|  4000|\n",
      "| E126|    Pom|  5000|      1| E126|  5000|\n",
      "| E125|    Som|  6000|      2| E125|  6000|\n",
      "| E124|    Rom|  7000|      2| E124|  7000|\n",
      "| E123|    Tom|  8000|      2| E123|  8000|\n",
      "| E123|    Tom|  2000|      1| E123|  2000|\n",
      "+-----+-------+------+-------+-----+------+\n",
      "\n",
      "+-----+-------+------+----+\n",
      "|EmpNo|EmpName|Salary|rank|\n",
      "+-----+-------+------+----+\n",
      "| E126|    Pom|  5000|   1|\n",
      "| E125|    Som|  4000|   1|\n",
      "| E125|    Som|  6000|   2|\n",
      "| E124|    Rom|  3000|   1|\n",
      "| E124|    Rom|  7000|   2|\n",
      "| E123|    Tom|  2000|   1|\n",
      "| E123|    Tom|  2000|   1|\n",
      "| E123|    Tom|  8000|   2|\n",
      "+-----+-------+------+----+\n",
      "\n",
      "+-----+-------+------+----+\n",
      "|EmpNo|EmpName|Salary|rank|\n",
      "+-----+-------+------+----+\n",
      "| E126|    Pom|  5000|   1|\n",
      "| E125|    Som|  4000|   1|\n",
      "| E125|    Som|  6000|   2|\n",
      "| E124|    Rom|  3000|   1|\n",
      "| E124|    Rom|  7000|   2|\n",
      "| E123|    Tom|  2000|   1|\n",
      "| E123|    Tom|  2000|   1|\n",
      "| E123|    Tom|  8000|   2|\n",
      "+-----+-------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def rowKeyFunction(x):\n",
    "    return x.Salary\n",
    "\n",
    "def ranking(x):\n",
    "    y=[]\n",
    "    for index,element in enumerate(x):\n",
    "        dictEle=element.asDict()\n",
    "        if(index==0):\n",
    "            dictEle['rank']=1\n",
    "        elif element.Salary==x[index-1].Salary:\n",
    "            dictEle['rank']=y[index-1]['rank']\n",
    "        else:\n",
    "            dictEle['rank']=y[index-1]['rank']+1\n",
    "        y.append(Row(**dictEle))\n",
    "    return y\n",
    "        \n",
    "empDf=sqlContext.read.csv(path='/home/kapil/software-apps/input-files/employee-input.csv',header=True,inferSchema=True)\n",
    "empDf.registerTempTable('employee')\n",
    "##rdd approach\n",
    "empRdd=empDf.rdd;\n",
    "windowPartitonRdd=empRdd.groupBy(lambda x:x['EmpNo'])\n",
    "sortedRdd=windowPartitonRdd.mapValues(lambda y:sorted(y,key=rowKeyFunction))\n",
    "rankedRdd=sortedRdd.mapValues(ranking)\n",
    "rowRdd=rankedRdd.values().flatMap(lambda x:x)\n",
    "createdDf=rowRdd.toDF()\n",
    "createdDf.show()\n",
    "\n",
    "\n",
    "##by usig window function\n",
    "windowSpec=Window.partitionBy(empDf['EmpNo']).orderBy(empDf['Salary'])\n",
    "##with joining two dataframes\n",
    "windowedDF=empDf.distinct().select(func.rank().over(windowSpec).alias('EmpRank'),empDf.EmpNo,empDf.Salary)\n",
    "joinedDF=empDf.join(other=windowedDF,on=([windowedDF.EmpNo==empDf.EmpNo,windowedDF.Salary==empDf.Salary]),how='inner')\n",
    "joinedDF.show()\n",
    "\n",
    "\n",
    "\n",
    "## using with column function and dense rank\n",
    "withColumnDf=empDf.withColumn(col=func.dense_rank().over(windowSpec),colName='rank')\n",
    "withColumnDf.show()\n",
    "\n",
    "\n",
    "##using sql\n",
    "sqlDf=sqlContext.sql('select EmpNo,EmpName,Salary, dense_rank() OVER (PARTITION BY EmpNo ORDER BY Salary ASC) as rank from employee')\n",
    "sqlDf.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Date Functions On DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-----+\n",
      "|  birthdate| id| name|\n",
      "+-----------+---+-----+\n",
      "|21-Mar-1991|123|kapil|\n",
      "|10-Dec-1986|124| Amit|\n",
      "+-----------+---+-----+\n",
      "\n",
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|1992-03-21|\n",
      "|1987-12-10|\n",
      "+----------+\n",
      "\n",
      "+----------+---+-----+\n",
      "| birthdate| id| name|\n",
      "+----------+---+-----+\n",
      "|1991-03-21|111|kapil|\n",
      "|1986-12-10|222| Amit|\n",
      "+----------+---+-----+\n",
      "\n",
      "+---------------+\n",
      "|Months_added_12|\n",
      "+---------------+\n",
      "|     1992-03-21|\n",
      "|     1987-12-10|\n",
      "+---------------+\n",
      "\n",
      "+-------------+\n",
      "|Days_added_30|\n",
      "+-------------+\n",
      "|   1991-04-20|\n",
      "|   1987-01-09|\n",
      "+-------------+\n",
      "\n",
      "+-------------------+\n",
      "|Days_substracted_30|\n",
      "+-------------------+\n",
      "|         1991-02-19|\n",
      "|         1986-11-10|\n",
      "+-------------------+\n",
      "\n",
      "+-----+------------------+\n",
      "| name|               age|\n",
      "+-----+------------------+\n",
      "|kapil|27.208219178082192|\n",
      "| Amit|31.487671232876714|\n",
      "+-----+------------------+\n",
      "\n",
      "+-----+------------------+\n",
      "| name|               age|\n",
      "+-----+------------------+\n",
      "|kapil|27.188172043333335|\n",
      "| Amit| 31.46774193583333|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format,unix_timestamp,to_date\n",
    "from pyspark.sql.functions import add_months,date_add,datediff,current_date,months_between\n",
    "\n",
    "import datetime\n",
    "\n",
    "class Employee():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name=''\n",
    "        self.id=0\n",
    "        self.birthdate=datetime.date\n",
    "\n",
    "    def setName(self,name):\n",
    "        self.name=name\n",
    "        \n",
    "    def setId(self,id):\n",
    "        self.id=id\n",
    "\n",
    "    def setbirthDate(self,bd):\n",
    "        self.birthdate=bd\n",
    "\n",
    "\n",
    "def mapEmployeeRow(x):\n",
    "    y=x.split(',')\n",
    "    d=y[2].split('/')\n",
    "    print(y)\n",
    "    emp=Employee()\n",
    "    emp.setId(y[0])\n",
    "    emp.setName(y[1])\n",
    "    emp.setbirthDate(datetime.date(int(d[0]),int(d[1]),int(d[2])))\n",
    "    return emp\n",
    "\n",
    "dataRdd=sc.parallelize(['111,kapil,1991/3/21','222,Amit,1986/12/10'])\n",
    "dataEmployeeRdd=dataRdd.map(mapEmployeeRow)\n",
    "##Need to import created class as separate file\n",
    "##dataEmployeeRdd.collect()\n",
    "\n",
    "##Data\n",
    "rows=sc.parallelize([Row(id=123,name='kapil',birthdate='21-Mar-1991'),\n",
    "                     Row(id=124,name='Amit',birthdate='10-Dec-1986')])\n",
    "##Schema with StringType and LongType\n",
    "schema=StructType([StructField('birthdate',StringType(),True),\n",
    "                       StructField('id',LongType(),True),\n",
    "                       StructField('name',StringType(),True)])\n",
    "rowsDf=spark.createDataFrame(data=rows,schema=schema)\n",
    "rowsDf.show()\n",
    "##adding months after converting String to date\n",
    "dateDf=rowsDf.select(add_months(to_date(unix_timestamp(rowsDf.birthdate,'dd-MMM-yyyy').cast(\"timestamp\"),'dd-MMM-yyyy'),12).alias('date'))\n",
    "dateDf.show()\n",
    "\n",
    "\n",
    "rdd=sc.parallelize(['111,kapil,1991/3/21','222,Amit,1986/12/10'])\n",
    "def mapRow(x):\n",
    "    y=x.split(',')\n",
    "    d=y[2].split('/')\n",
    "    return Row(id=y[0],name=y[1],birthdate=datetime.date(int(d[0]),int(d[1]),int(d[2])))\n",
    "\n",
    "dateDfs=spark.createDataFrame(rdd.map(mapRow))\n",
    "dateDfs.show()\n",
    "\n",
    "##Having DateType Column in Dataframe\n",
    "dateRows=sc.parallelize([Row(id=123,name='kapil',birthdate=datetime.date(1991,3,21)),\n",
    "                     Row(id=124,name='Amit',birthdate=datetime.date(1986,12,10))])\n",
    "schema2=StructType([StructField('birthdate',DateType(),True),\n",
    "                       StructField('id',LongType(),True),\n",
    "                       StructField('name',StringType(),True)])\n",
    "\n",
    "dateRowsDf=spark.createDataFrame(data=dateRows,schema=schema2)\n",
    "\n",
    "##add months\n",
    "dateRowsDf.select(add_months(dateRowsDf.birthdate,12).alias('Months_added_12')).show()\n",
    "## add days\n",
    "dateRowsDf.select(date_add(dateRowsDf.birthdate,30).alias('Days_added_30')).show()\n",
    "## substract days\n",
    "dateRowsDf.select(date_add(dateRowsDf.birthdate,-30).alias('Days_substracted_30')).show()\n",
    "## diffrence between dates\n",
    "dateRowsDf.select(dateRowsDf.name,(datediff(current_date(),dateRowsDf.birthdate)/365).alias('age')).show()\n",
    "##months between\n",
    "dateRowsDf.select(dateRowsDf.name,(months_between(current_date(),dateRowsDf.birthdate)/12).alias('age')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
