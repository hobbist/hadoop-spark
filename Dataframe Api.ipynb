{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name| id|\n",
      "+----+---+\n",
      "|   1|  3|\n",
      "|   2|  4|\n",
      "|   3|  5|\n",
      "|   4|  6|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##WithColumn example\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "rdd=sc.parallelize([1,2,3,4])\n",
    "rdd.collect()\n",
    "rowRdd=rdd.map(lambda x: Row(name=x))\n",
    "schema = StructType([StructField(\"name\", IntegerType(), False)])\n",
    "df=sqlContext.createDataFrame(data=rowRdd,schema=schema)\n",
    "df2=df.select('name')\n",
    "df3=df.withColumn('id',df.name+2)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|avg(value1)|\n",
      "+---+-----------+\n",
      "|  b|        2.0|\n",
      "|  a|        2.0|\n",
      "+---+-----------+\n",
      "\n",
      "+-----------+---+\n",
      "|avg(value1)| id|\n",
      "+-----------+---+\n",
      "|        2.0|  b|\n",
      "|        2.0|  a|\n",
      "+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datafrm=sc.parallelize([Row(id='a',value1=1,value2=2),Row(id='b',value1=2,value2=3),Row(id='a',value1=3,value2=4)]).toDF()\n",
    "computedOne=datafrm.withColumn('newCol',datafrm.value1*datafrm.value2)\n",
    "averagedDataFrame=datafrm.groupBy('id').avg('value1')\n",
    "averagedDataFrame.show()\n",
    "sqlContext.registerDataFrameAsTable(df=datafrm,tableName='values')\n",
    "avgDF=sqlContext.sql('select avg(value1),id from values group by id')\n",
    "avgDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-------------------+\n",
      "|                dat|name|           prevDate|\n",
      "+-------------------+----+-------------------+\n",
      "|2018-04-20 00:00:00|   a|2018-04-19 00:00:00|\n",
      "|2018-04-20 00:00:00|   b|2018-04-19 00:00:00|\n",
      "|2018-04-20 00:00:00|   a|2018-04-19 00:00:00|\n",
      "+-------------------+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Calculating previous Date in dataFrame\n",
    "from datetime import datetime,timedelta\n",
    "from pyspark.sql.functions import when,date_add,col\n",
    "dateDF=sc.parallelize([Row(name='a',dat=datetime(2018,4,20)),\n",
    "                       Row(name='b',dat=datetime(2018,4,20)),\n",
    "                       Row(name='a',dat=datetime(2018,4,20))]).toDF()\n",
    "addedColumn=dateDF.withColumn('prevDate',when(dateDF.dat.isNotNull(),date_add(col('dat'),-1)).otherwise(dateDF['dat']))\n",
    "addedColumn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n",
      "+-------+\n",
      "|avg(id)|\n",
      "+-------+\n",
      "|    7.5|\n",
      "+-------+\n",
      "\n",
      "+---+---+\n",
      "| id| id|\n",
      "+---+---+\n",
      "|  1|  6|\n",
      "|  1|  7|\n",
      "|  1|  8|\n",
      "|  1|  9|\n",
      "|  2|  6|\n",
      "|  2|  7|\n",
      "|  2|  8|\n",
      "|  2|  9|\n",
      "|  3|  6|\n",
      "|  3|  7|\n",
      "|  3|  8|\n",
      "|  3|  9|\n",
      "|  4|  6|\n",
      "|  4|  7|\n",
      "|  4|  8|\n",
      "|  4|  9|\n",
      "|  5|  6|\n",
      "|  5|  7|\n",
      "|  5|  8|\n",
      "|  5|  9|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+------------------+------------------+\n",
      "|summary|                id|                id|\n",
      "+-------+------------------+------------------+\n",
      "|  count|                36|                36|\n",
      "|   mean|               5.0|               7.5|\n",
      "| stddev|2.6186146828319083|1.1338934190276817|\n",
      "|    min|                 1|                 6|\n",
      "|    max|                 9|                 9|\n",
      "+-------+------------------+------------------+\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "++\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "##range to generate df\n",
    "rangeDF=spark.range(1,10,1)\n",
    "rangeDF.show()\n",
    "##data frame select and where function\n",
    "filteredDF=rangeDF.select('id').where(rangeDF.id>5)\n",
    "filteredDF.show()\n",
    "#agg function\n",
    "maxId=filteredDF.agg(F.avg(filteredDF.id))\n",
    "maxId.show()\n",
    "##approxquantile\n",
    "#quartDF=rangeDF.approxQuantile('id',[0,1],0.5)\n",
    "#quartDF.show()\n",
    "rangeDF.columns\n",
    "\n",
    "##cross join\n",
    "crossDF=rangeDF.crossJoin(filteredDF)\n",
    "crossDF.show()\n",
    "crossDF.describe().show()\n",
    "crossDF.drop('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+----+\n",
      "|EmpName|EmpNo|Salary|rank|\n",
      "+-------+-----+------+----+\n",
      "|    Som| E125|  4000|   1|\n",
      "|    Som| E125|  6000|   2|\n",
      "|    Tom| E123|  2000|   1|\n",
      "|    Tom| E123|  2000|   1|\n",
      "|    Tom| E123|  8000|   2|\n",
      "|    Rom| E124|  3000|   1|\n",
      "|    Rom| E124|  7000|   2|\n",
      "|    Pom| E126|  5000|   1|\n",
      "+-------+-----+------+----+\n",
      "\n",
      "+-----+-------+------+-------+-----+------+\n",
      "|EmpNo|EmpName|Salary|EmpRank|EmpNo|Salary|\n",
      "+-----+-------+------+-------+-----+------+\n",
      "| E123|    Tom|  2000|      1| E123|  2000|\n",
      "| E124|    Rom|  3000|      1| E124|  3000|\n",
      "| E125|    Som|  4000|      1| E125|  4000|\n",
      "| E126|    Pom|  5000|      1| E126|  5000|\n",
      "| E125|    Som|  6000|      2| E125|  6000|\n",
      "| E124|    Rom|  7000|      2| E124|  7000|\n",
      "| E123|    Tom|  8000|      2| E123|  8000|\n",
      "| E123|    Tom|  2000|      1| E123|  2000|\n",
      "+-----+-------+------+-------+-----+------+\n",
      "\n",
      "+-----+-------+------+----+\n",
      "|EmpNo|EmpName|Salary|rank|\n",
      "+-----+-------+------+----+\n",
      "| E126|    Pom|  5000|   1|\n",
      "| E125|    Som|  4000|   1|\n",
      "| E125|    Som|  6000|   2|\n",
      "| E124|    Rom|  3000|   1|\n",
      "| E124|    Rom|  7000|   2|\n",
      "| E123|    Tom|  2000|   1|\n",
      "| E123|    Tom|  2000|   1|\n",
      "| E123|    Tom|  8000|   2|\n",
      "+-----+-------+------+----+\n",
      "\n",
      "+-----+-------+------+----+\n",
      "|EmpNo|EmpName|Salary|rank|\n",
      "+-----+-------+------+----+\n",
      "| E126|    Pom|  5000|   1|\n",
      "| E125|    Som|  4000|   1|\n",
      "| E125|    Som|  6000|   2|\n",
      "| E124|    Rom|  3000|   1|\n",
      "| E124|    Rom|  7000|   2|\n",
      "| E123|    Tom|  2000|   1|\n",
      "| E123|    Tom|  2000|   1|\n",
      "| E123|    Tom|  8000|   2|\n",
      "+-----+-------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def rowKeyFunction(x):\n",
    "    return x.Salary\n",
    "\n",
    "def ranking(x):\n",
    "    y=[]\n",
    "    for index,element in enumerate(x):\n",
    "        dictEle=element.asDict()\n",
    "        if(index==0):\n",
    "            dictEle['rank']=1\n",
    "        elif element.Salary==x[index-1].Salary:\n",
    "            dictEle['rank']=y[index-1]['rank']\n",
    "        else:\n",
    "            dictEle['rank']=y[index-1]['rank']+1\n",
    "        y.append(Row(**dictEle))\n",
    "    return y\n",
    "        \n",
    "empDf=sqlContext.read.csv(path='/home/kapil/software-apps/input-files/employee-input.csv',header=True,inferSchema=True)\n",
    "empDf.registerTempTable('employee')\n",
    "##rdd approach\n",
    "empRdd=empDf.rdd;\n",
    "windowPartitonRdd=empRdd.groupBy(lambda x:x['EmpNo'])\n",
    "sortedRdd=windowPartitonRdd.mapValues(lambda y:sorted(y,key=rowKeyFunction))\n",
    "rankedRdd=sortedRdd.mapValues(ranking)\n",
    "rowRdd=rankedRdd.values().flatMap(lambda x:x)\n",
    "createdDf=rowRdd.toDF()\n",
    "createdDf.show()\n",
    "\n",
    "\n",
    "##by usig window function\n",
    "windowSpec=Window.partitionBy(empDf['EmpNo']).orderBy(empDf['Salary'])\n",
    "##with joining two dataframes\n",
    "windowedDF=empDf.distinct().select(func.rank().over(windowSpec).alias('EmpRank'),empDf.EmpNo,empDf.Salary)\n",
    "joinedDF=empDf.join(other=windowedDF,on=([windowedDF.EmpNo==empDf.EmpNo,windowedDF.Salary==empDf.Salary]),how='inner')\n",
    "joinedDF.show()\n",
    "\n",
    "\n",
    "\n",
    "## using with column function and dense rank\n",
    "withColumnDf=empDf.withColumn(col=func.dense_rank().over(windowSpec),colName='rank')\n",
    "withColumnDf.show()\n",
    "\n",
    "\n",
    "##using sql\n",
    "sqlDf=sqlContext.sql('select EmpNo,EmpName,Salary, dense_rank() OVER (PARTITION BY EmpNo ORDER BY Salary ASC) as rank from employee')\n",
    "sqlDf.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Date Functions On DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 454.0 failed 1 times, most recent failure: Lost task 3.0 in stage 454.0 (TID 2042, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/kapil/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/kapil/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/kapil/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 376, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/home/kapil/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 555, in dumps\n    return pickle.dumps(obj, protocol)\n_pickle.PicklingError: Can't pickle <class '__main__.Employee'>: attribute lookup Employee on __main__ failed\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/kapil/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/kapil/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/kapil/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 376, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/home/kapil/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 555, in dumps\n    return pickle.dumps(obj, protocol)\n_pickle.PicklingError: Can't pickle <class '__main__.Employee'>: attribute lookup Employee on __main__ failed\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-acd7c9a647dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mdataRdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'111,kapil,1991/3/21'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'222,Amit,1986/12/10'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mdataEmployeeRdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataRdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapEmployeeRow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mdataEmployeeRdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;31m##Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m rows=sc.parallelize([Row(id=123,name='kapil',birthdate='21-Mar-1991'),\n",
      "\u001b[0;32m~/software-apps/spark-2.3.0-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \"\"\"\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software-apps/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 454.0 failed 1 times, most recent failure: Lost task 3.0 in stage 454.0 (TID 2042, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/kapil/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/kapil/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/kapil/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 376, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/home/kapil/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 555, in dumps\n    return pickle.dumps(obj, protocol)\n_pickle.PicklingError: Can't pickle <class '__main__.Employee'>: attribute lookup Employee on __main__ failed\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/kapil/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/kapil/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/kapil/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 376, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/home/kapil/software-apps/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 555, in dumps\n    return pickle.dumps(obj, protocol)\n_pickle.PicklingError: Can't pickle <class '__main__.Employee'>: attribute lookup Employee on __main__ failed\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format,unix_timestamp,to_date\n",
    "from pyspark.sql.functions import add_months,date_add,datediff,current_date,months_between\n",
    "\n",
    "import datetime\n",
    "\n",
    "class Employee():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name=''\n",
    "        self.id=0\n",
    "        self.birthdate=datetime.date\n",
    "\n",
    "    def setName(self,name):\n",
    "        self.name=name\n",
    "        \n",
    "    def setId(self,id):\n",
    "        self.id=id\n",
    "\n",
    "    def setbirthDate(self,bd):\n",
    "        self.birthdate=bd\n",
    "\n",
    "\n",
    "def mapEmployeeRow(x):\n",
    "    y=x.split(',')\n",
    "    d=y[2].split('/')\n",
    "    print(y)\n",
    "    emp=Employee()\n",
    "    emp.setId(y[0])\n",
    "    emp.setName(y[1])\n",
    "    emp.setbirthDate(datetime.date(int(d[0]),int(d[1]),int(d[2])))\n",
    "    return emp\n",
    "\n",
    "dataRdd=sc.parallelize(['111,kapil,1991/3/21','222,Amit,1986/12/10'])\n",
    "dataEmployeeRdd=dataRdd.map(mapEmployeeRow)\n",
    "##Need to import created class as separate file\n",
    "##dataEmployeeRdd.collect()\n",
    "\n",
    "##Data\n",
    "rows=sc.parallelize([Row(id=123,name='kapil',birthdate='21-Mar-1991'),\n",
    "                     Row(id=124,name='Amit',birthdate='10-Dec-1986')])\n",
    "##Schema with StringType and LongType\n",
    "schema=StructType([StructField('birthdate',StringType(),True),\n",
    "                       StructField('id',LongType(),True),\n",
    "                       StructField('name',StringType(),True)])\n",
    "rowsDf=spark.createDataFrame(data=rows,schema=schema)\n",
    "rowsDf.show()\n",
    "##adding months after converting String to date\n",
    "dateDf=rowsDf.select(add_months(to_date(unix_timestamp(rowsDf.birthdate,'dd-MMM-yyyy').cast(\"timestamp\"),'dd-MMM-yyyy'),12).alias('date'))\n",
    "dateDf.show()\n",
    "\n",
    "\n",
    "rdd=sc.parallelize(['111,kapil,1991/3/21','222,Amit,1986/12/10'])\n",
    "def mapRow(x):\n",
    "    y=x.split(',')\n",
    "    d=y[2].split('/')\n",
    "    return Row(id=y[0],name=y[1],birthdate=datetime.date(int(d[0]),int(d[1]),int(d[2])))\n",
    "\n",
    "dateDfs=spark.createDataFrame(rdd.map(mapRow))\n",
    "dateDfs.show()\n",
    "\n",
    "##Having DateType Column in Dataframe\n",
    "dateRows=sc.parallelize([Row(id=123,name='kapil',birthdate=datetime.date(1991,3,21)),\n",
    "                     Row(id=124,name='Amit',birthdate=datetime.date(1986,12,10))])\n",
    "schema2=StructType([StructField('birthdate',DateType(),True),\n",
    "                       StructField('id',LongType(),True),\n",
    "                       StructField('name',StringType(),True)])\n",
    "\n",
    "dateRowsDf=spark.createDataFrame(data=dateRows,schema=schema2)\n",
    "\n",
    "##add months\n",
    "dateRowsDf.select(add_months(dateRowsDf.birthdate,12).alias('Months_added_12')).show()\n",
    "## add days\n",
    "dateRowsDf.select(date_add(dateRowsDf.birthdate,30).alias('Days_added_30')).show()\n",
    "## substract days\n",
    "dateRowsDf.select(date_add(dateRowsDf.birthdate,-30).alias('Days_substracted_30')).show()\n",
    "## diffrence between dates\n",
    "dateRowsDf.select(dateRowsDf.name,(datediff(current_date(),dateRowsDf.birthdate)/365).alias('age')).show()\n",
    "##months between\n",
    "dateRowsDf.select(dateRowsDf.name,(months_between(current_date(),dateRowsDf.birthdate)/12).alias('age')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
