{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: timestamp, temperature: double, humidity: double]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "\n",
    "##function to get default string value to avoid exception of casting empty string to float\n",
    "def getString(v):\n",
    "    if (v!=''):\n",
    "        return v\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "## function to get Row objects for each json key value pair as timestamp and temperature\n",
    "def createTRows(jsn):\n",
    "    arr=list()\n",
    "    for k in jsn:\n",
    "        arr.append(Row(timestamp=datetime.strptime(k,'%Y-%m-%dT%H:%M:%S'),temperature=float(getString(jsn[k]))))\n",
    "    return arr\n",
    "\n",
    "## function to get Row objects for each json key value pair as timestamp and humidity\n",
    "def createHRows(jsn):\n",
    "    arr=list()\n",
    "    for k in jsn:\n",
    "        arr.append(Row(timestamp=datetime.strptime(k,'%Y-%m-%dT%H:%M:%S'),humidity=float(getString(jsn[k]))))\n",
    "    return arr\n",
    "\n",
    "## getting data from files tempm.txt,hum.txt\n",
    "## each line in file is valid json in below format\n",
    "## {“timestamp1”: “value1”, “timestamp2”: “value2”,“timestampN”: “valueN”}\n",
    "## converting each row of file in rdd of json objects\n",
    "temperatureRdd=sc.textFile(\"file:///C:/datapath/spark files/tempm.txt\")\n",
    "tempJsonRdd=temperatureRdd.map(lambda x: json.loads(x))\n",
    "humidityRdd=sc.textFile(\"file:///C:/datapath/spark files/hum.txt\")\n",
    "humidityJsonRdd=humidityRdd.map(lambda x: json.loads(x))\n",
    "\n",
    "## converting each timestamp:value pair in DF of timestamp,temperature and humidity,temperature\n",
    "temperatureDF=tempJsonRdd.flatMap(lambda x: createTRows(x)).toDF()\n",
    "humidityDF=humidityJsonRdd.flatMap(lambda x: createHRows(x)).toDF()\n",
    "\n",
    "## joining above created dataframes in one using inner join.Final DF will have timestamp,temperature,humidity columns\n",
    "joinedDF=temperatureDF.join(humidityDF,'timestamp')\n",
    "##persisting the joined DF\n",
    "joinedDF.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|to_date(timestamp)|\n",
      "+------------------+\n",
      "|        2014-06-03|\n",
      "|        2014-04-29|\n",
      "|        2014-05-24|\n",
      "|        2014-06-05|\n",
      "|        2014-05-30|\n",
      "|        2014-05-25|\n",
      "|        2014-05-26|\n",
      "|        2014-05-21|\n",
      "|        2014-06-02|\n",
      "|        2014-05-17|\n",
      "|        2014-05-22|\n",
      "|        2014-06-04|\n",
      "|        2014-04-28|\n",
      "|        2014-05-23|\n",
      "|        2014-06-01|\n",
      "|        2014-06-07|\n",
      "|        2014-06-08|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##In how many days has the temperature reached or exceeded 20 ° C\n",
    "from pyspark.sql.functions import to_date\n",
    "query1DF=joinedDF.select(to_date(joinedDF.timestamp)).where(joinedDF.temperature>=20).distinct()\n",
    "query1DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|to_date(timestamp)|  avg(temperature)|\n",
      "+------------------+------------------+\n",
      "|        2014-05-22|              20.0|\n",
      "|        2014-05-21| 17.63888888888889|\n",
      "|        2014-06-08| 17.63888888888889|\n",
      "|        2014-05-23|16.257142857142856|\n",
      "|        2014-06-02|16.056338028169016|\n",
      "|        2014-05-24|15.833333333333334|\n",
      "|        2014-06-07|15.666666666666666|\n",
      "|        2014-05-25|15.619718309859154|\n",
      "|        2014-06-03| 15.61111111111111|\n",
      "|        2014-06-04|15.485714285714286|\n",
      "|        2014-06-05|15.430555555555555|\n",
      "|        2014-05-30|15.112676056338028|\n",
      "|        2014-06-01|15.027777777777779|\n",
      "|        2014-05-27| 14.48611111111111|\n",
      "|        2014-05-26| 14.46376811594203|\n",
      "|        2014-05-20|14.083333333333334|\n",
      "|        2014-05-19|13.808823529411764|\n",
      "|        2014-05-18|             13.75|\n",
      "|        2014-04-22|13.722222222222221|\n",
      "|        2014-05-17|13.541666666666666|\n",
      "+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Which were the 20 hottest days?\n",
    "## Here hottest day is considered based on maximum value of average temeprature for the day \n",
    "from pyspark.sql.functions import desc\n",
    "query2DF=joinedDF.groupBy(to_date(joinedDF.timestamp)).avg('temperature').orderBy(desc('avg(temperature)'))\n",
    "query2DF.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------------+\n",
      "|month(timestamp)|stddev_samp(temperature)|\n",
      "+----------------+------------------------+\n",
      "|               5|       4.426954778558993|\n",
      "+----------------+------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------------+---------------------+\n",
      "|month(timestamp)|stddev_samp(humidity)|\n",
      "+----------------+---------------------+\n",
      "|               4|    17.83959640684249|\n",
      "+----------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Which month did the highest standard deviation in humidity/moisture values?\n",
    "from pyspark.sql.functions import stddev,month\n",
    "##maximum standard deviation for temperature\n",
    "query3TDF=joinedDF.groupBy(month(joinedDF.timestamp)).agg(stddev('temperature')).orderBy(desc('stddev_samp(temperature)'))\n",
    "query3TDF.show(1)\n",
    "##maximum standard deviation for humidity\n",
    "query3HDF=joinedDF.groupBy(month(joinedDF.timestamp)).agg(stddev('humidity')).orderBy(desc('stddev_samp(humidity)'))\n",
    "query3HDF.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|max(DI)|            min(DI)|\n",
      "+-------+-------------------+\n",
      "|22.1125|-2.3262500000000004|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max,min\n",
    "## caculating distortion index for each contact\n",
    "distortionDF=joinedDF.withColumn('DI',joinedDF.temperature-0.55*(1-0.01*joinedDF.humidity)*(joinedDF.temperature-14.5))\n",
    "## min and max value extracted in DF\n",
    "query4DF=distortionDF.select(max('DI'),min('DI'))\n",
    "query4DF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
